{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d218836",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraris\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3619a679",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipaddress\n",
    "\n",
    "# Load main data and IP-country mapping\n",
    "df = pd.read_csv(r'C:\\Users\\admin\\fraud-detection-week5\\data\\raw\\Fraud_Data.csv')  # replace with your file\n",
    "ip_df = pd.read_csv(r'C:\\Users\\admin\\fraud-detection-week5\\data\\raw\\IpAddress_to_Country.csv')\n",
    "\n",
    "print(\"Main data columns:\", df.columns.tolist())\n",
    "print(\"IP mapping columns:\", ip_df.columns.tolist())\n",
    "\n",
    "# Step 1: Convert main df's ip_address to integer\n",
    "def ip_to_int(ip):\n",
    "    try:\n",
    "        return int(ipaddress.ip_address(ip))\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "df['ip_int'] = df['ip_address'].apply(ip_to_int)\n",
    "\n",
    "# Step 2: Ensure IP range bounds in ip_df are integers (they usually are, but confirm)\n",
    "# If they're strings, convert them — but typically they're already ints\n",
    "print(\"Sample IP bounds:\", ip_df[['lower_bound_ip_address', 'upper_bound_ip_address']].head())\n",
    "\n",
    "# Step 3: Function to map IP int to country using ip_df\n",
    "def find_country(ip_int):\n",
    "    if pd.isna(ip_int):\n",
    "        return 'Unknown'\n",
    "    # Search for matching range\n",
    "    match = ip_df[\n",
    "        (ip_df['lower_bound_ip_address'] <= ip_int) &\n",
    "        (ip_df['upper_bound_ip_address'] >= ip_int)\n",
    "    ]\n",
    "    if not match.empty:\n",
    "        return match.iloc[0]['country']\n",
    "    return 'Unknown'\n",
    "\n",
    "# Step 4: Apply to main DataFrame\n",
    "df['country'] = df['ip_int'].apply(find_country)\n",
    "print(f\"Countries mapped: {df['country'].nunique()}\")\n",
    "\n",
    "# Step 5: Analyze fraud by country (assuming 'class' = 1 means fraud)\n",
    "country_fraud = df.groupby('country')['class'].agg(['count', 'mean', 'sum']).sort_values('mean', ascending=False)\n",
    "print(\"\\nTop 10 High Fraud Rate Countries:\")\n",
    "print(country_fraud.head(10))\n",
    "\n",
    "# Step 6: Plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "top_20 = country_fraud.head(20)\n",
    "plt.barh(top_20.index, top_20['mean'] * 100, color='crimson')\n",
    "plt.xlabel('Fraud Rate (%)')\n",
    "plt.title('Top 20 Countries by Fraud Rate')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.savefig(r'C:\\Users\\admin\\fraud-detection-week5\\reports\\fraud_by_country.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae550f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# ---- 0. Ensure datetime columns ----\n",
    "df['purchase_time'] = pd.to_datetime(df['purchase_time'])\n",
    "df['signup_time'] = pd.to_datetime(df['signup_time'])\n",
    "\n",
    "# ---- 1. Time-based features ----\n",
    "df['hour_of_day'] = df['purchase_time'].dt.hour\n",
    "df['day_of_week'] = df['purchase_time'].dt.dayofweek\n",
    "df['is_weekend'] = df['day_of_week'].isin([5, 6]).astype(int)\n",
    "\n",
    "# ---- 2. Time since signup ----\n",
    "df['time_since_signup'] = (df['purchase_time'] - df['signup_time']).dt.total_seconds() / 3600\n",
    "\n",
    "# ---- 3. Transaction frequency & velocity features ----\n",
    "df = df.sort_values(['user_id', 'purchase_time', 'purchase_value'])\n",
    "\n",
    "# Time since last transaction per user\n",
    "df['time_since_last_txn'] = df.groupby('user_id')['purchase_time'].diff().dt.total_seconds() / 60\n",
    "\n",
    "# ---- Robust rolling counts & sums ----\n",
    "# We'll use a helper function to avoid duplicate datetime issues\n",
    "def rolling_feature(group, value_col, window, agg='count'):\n",
    "    group = group.copy()\n",
    "    group = group.sort_values('purchase_time')\n",
    "    # Add a tiny offset to duplicates to make index unique\n",
    "    group['purchase_time_index'] = group['purchase_time'] + pd.to_timedelta(range(len(group)), unit='ns')\n",
    "    group = group.set_index('purchase_time_index')\n",
    "    if agg == 'count':\n",
    "        return group[value_col].rolling(window).count().values\n",
    "    elif agg == 'sum':\n",
    "        return group[value_col].rolling(window).sum().values\n",
    "\n",
    "# Rolling transaction count in last 24h and 7d\n",
    "df['txn_count_24h'] = df.groupby('user_id', group_keys=False).apply(lambda x: rolling_feature(x, 'user_id', '24h', 'count'))\n",
    "df['txn_count_7d'] = df.groupby('user_id', group_keys=False).apply(lambda x: rolling_feature(x, 'user_id', '7d', 'count'))\n",
    "\n",
    "# Purchase velocity in last 1h\n",
    "df['purchase_velocity_1h'] = df.groupby('user_id', group_keys=False).apply(lambda x: rolling_feature(x, 'purchase_value', '1h', 'sum'))\n",
    "\n",
    "# Average purchase per user\n",
    "df['avg_purchase_user'] = df.groupby('user_id')['purchase_value'].transform('mean')\n",
    "\n",
    "# Unique devices per user\n",
    "df['unique_devices_per_user'] = df.groupby('user_id')['device_id'].transform('nunique')\n",
    "\n",
    "# ---- Summary ----\n",
    "print(f\"Total features created: {len(df.columns)}\")\n",
    "print(f\"New features: hour_of_day, day_of_week, is_weekend, time_since_signup, \"\n",
    "      f\"time_since_last_txn, txn_count_24h, txn_count_7d, \"\n",
    "      f\"purchase_velocity_1h, avg_purchase_user, unique_devices_per_user\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37efd96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Separate features and target\n",
    "X = df.drop('class', axis=1)\n",
    "y = df['class']\n",
    "\n",
    "# Identify column types\n",
    "categorical_cols = ['source', 'browser', 'sex', 'country', 'deviceid']\n",
    "numerical_cols = ['purchasevalue', 'age', 'time_since_signup', 'time_since_last_txn',\n",
    "                  'txn_count_24h', 'txn_count_7d', 'purchase_velocity_1h',\n",
    "                  'avg_purchase_user', 'unique_devices_per_user']\n",
    "\n",
    "# Remove identifiers and datetime from modeling\n",
    "exclude_cols = ['userid', 'signuptime', 'purchasetime', 'ipaddress', 'ip_int', 'purchase_hour']\n",
    "X_model = X.drop(columns=exclude_cols, errors='ignore')\n",
    "\n",
    "# Update column lists after dropping\n",
    "categorical_cols = [col for col in categorical_cols if col in X_model.columns]\n",
    "numerical_cols = [col for col in numerical_cols if col in X_model.columns]\n",
    "\n",
    "# Create preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_cols),\n",
    "        ('cat', OneHotEncoder(drop='first', sparse_output=False, handle_unknown='ignore'), categorical_cols)\n",
    "    ])\n",
    "\n",
    "# Fit and transform\n",
    "X_processed = preprocessor.fit_transform(X_model)\n",
    "print(f\"Processed shape: {X_processed.shape}\")\n",
    "\n",
    "# Get feature names\n",
    "cat_encoder = preprocessor.named_transformers_['cat']\n",
    "cat_features = cat_encoder.get_feature_names_out(categorical_cols)\n",
    "all_features = list(numerical_cols) + list(cat_features)\n",
    "print(f\"Total features after encoding: {len(all_features)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99c61ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Train-test split FIRST (IMPORTANT: never apply SMOTE before splitting!)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_processed, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"CLASS IMBALANCE HANDLING STRATEGY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nBefore resampling:\")\n",
    "print(f\"Training set shape: {X_train.shape}\")\n",
    "print(f\"Fraud cases in training: {y_train.sum()} ({y_train.mean()*100:.2f}%)\")\n",
    "print(f\"Non-fraud cases in training: {len(y_train)-y_train.sum()}\")\n",
    "\n",
    "# DECISION: Use SMOTE for moderate oversampling + RandomUnderSampling for majority\n",
    "# Justification written in markdown cell:\n",
    "\"\"\"\n",
    "## Justification for Imbalance Handling Strategy\n",
    "\n",
    "**Why SMOTE?**\n",
    "- Our fraud rate is extremely low (~1-2% typical for fraud data)\n",
    "- SMOTE creates synthetic minority samples instead of duplicating\n",
    "- Helps prevent overfitting compared to random oversampling\n",
    "- Preserves the information in the majority class\n",
    "\n",
    "**Why combine with RandomUnderSampling?**\n",
    "- Pure SMOTE on severe imbalance can lead to overgeneralization\n",
    "- Under-sampling majority reduces computational cost\n",
    "- Combination (SMOTEENN or pipeline) often gives better decision boundaries\n",
    "\n",
    "**Alternative considered:**\n",
    "- Class weighting in models: Good, but doesn't create new patterns\n",
    "- ADASYN: Similar to SMOTE but focuses on difficult samples\n",
    "\n",
    "**Final decision:** SMOTE to 10% fraud rate, then light under-sampling\n",
    "\"\"\"\n",
    "\n",
    "# Apply SMOTE (oversample minority)\n",
    "smote = SMOTE(sampling_strategy=0.1, random_state=42)  # Target 10% fraud rate\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "print(f\"\\nAfter SMOTE (10% fraud rate target):\")\n",
    "print(f\"Training set shape: {X_train_smote.shape}\")\n",
    "print(f\"Fraud cases: {y_train_smote.sum()} ({y_train_smote.mean()*100:.2f}%)\")\n",
    "\n",
    "# Optional: Light under-sampling of majority\n",
    "rus = RandomUnderSampler(sampling_strategy=0.5, random_state=42)  # 2:1 ratio\n",
    "X_train_balanced, y_train_balanced = rus.fit_resample(X_train_smote, y_train_smote)\n",
    "\n",
    "print(f\"\\nAfter final balancing:\")\n",
    "print(f\"Training set shape: {X_train_balanced.shape}\")\n",
    "print(f\"Fraud rate: {y_train_balanced.mean()*100:.2f}%\")\n",
    "print(f\"Class distribution: {pd.Series(y_train_balanced).value_counts().to_dict()}\")\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Before\n",
    "axes[0].pie([len(y_train)-y_train.sum(), y_train.sum()], \n",
    "            labels=['Non-Fraud', 'Fraud'], autopct='%1.1f%%',\n",
    "            colors=['#3498db', '#e74c3c'])\n",
    "axes[0].set_title('Before Resampling')\n",
    "\n",
    "# After SMOTE\n",
    "axes[1].pie([len(y_train_smote)-y_train_smote.sum(), y_train_smote.sum()],\n",
    "            labels=['Non-Fraud', 'Fraud'], autopct='%1.1f%%',\n",
    "            colors=['#3498db', '#e74c3c'])\n",
    "axes[1].set_title('After SMOTE (10%)')\n",
    "\n",
    "# Final\n",
    "axes[2].pie([len(y_train_balanced)-y_train_balanced.sum(), y_train_balanced.sum()],\n",
    "            labels=['Non-Fraud', 'Fraud'], autopct='%1.1f%%',\n",
    "            colors=['#3498db', '#e74c3c'])\n",
    "axes[2].set_title('Final Balanced (2:1)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../reports/class_balance_evolution.png', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# Save processed data\n",
    "processed_data = {\n",
    "    'X_train': X_train_balanced,\n",
    "    'X_test': X_test,\n",
    "    'y_train': y_train_balanced,\n",
    "    'y_test': y_test,\n",
    "    'feature_names': all_features,\n",
    "    'preprocessor': preprocessor\n",
    "}\n",
    "\n",
    "import joblib\n",
    "joblib.dump(processed_data, r'C:/Users/admin/fraud-detection-week5/data/processed/processed/train_test_data.pkl')\n",
    "print(\"\\n✓ Processed data saved to ../data/processed/train_test_data.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
