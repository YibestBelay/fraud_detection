{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de551570",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fraud Detection - Feature Engineering Notebook\n",
    "# Task 1: Data Analysis and Preprocessing\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add src to path\n",
    "sys.path.append('../src')\n",
    "\n",
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n",
    "\n",
    "# Import custom modules\n",
    "from data_cleaning import FraudDataCleaner\n",
    "from eda import FraudEDA\n",
    "from geolocation import IPGeolocationMapper\n",
    "from feature_engineering import FraudFeatureEngineer\n",
    "from data_transformation import FraudDataTransformer\n",
    "\n",
    "print(\"All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6794f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load and Clean Data\n",
    "print(\"=\"*80)\n",
    "print(\"STEP 1: DATA LOADING AND CLEANING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Initialize cleaner\n",
    "cleaner = FraudDataCleaner(verbose=True)\n",
    "\n",
    "# Load fraud data\n",
    "fraud_data_path = '../data/raw/Fraud_Data.csv'\n",
    "df_fraud = cleaner.load_data(fraud_data_path)\n",
    "\n",
    "# Display initial info\n",
    "print(f\"\\nðŸ“Š Dataset Information:\")\n",
    "print(f\"Shape: {df_fraud.shape}\")\n",
    "print(f\"Columns: {df_fraud.columns.tolist()}\")\n",
    "print(f\"\\nSample data:\")\n",
    "print(df_fraud.head())\n",
    "\n",
    "# Comprehensive cleaning\n",
    "df_fraud_clean = cleaner.clean_fraud_data(df_fraud)\n",
    "\n",
    "# Generate cleaning report\n",
    "cleaning_report = cleaner.generate_cleaning_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d8d307",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Exploratory Data Analysis\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 2: EXPLORATORY DATA ANALYSIS (EDA)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Initialize EDA\n",
    "eda = FraudEDA(figsize=(12, 8))\n",
    "\n",
    "# 2.1 Class Distribution Analysis\n",
    "class_stats = eda.analyze_class_distribution(df_fraud_clean, target_col='class')\n",
    "\n",
    "# 2.2 Univariate Analysis\n",
    "eda.univariate_analysis(df_fraud_clean)\n",
    "\n",
    "# 2.3 Bivariate Analysis\n",
    "eda.bivariate_analysis(df_fraud_clean, target_col='class')\n",
    "\n",
    "# 2.4 Correlation Analysis\n",
    "eda.correlation_analysis(df_fraud_clean)\n",
    "\n",
    "# 2.5 Outlier Analysis\n",
    "eda.outlier_analysis(df_fraud_clean)\n",
    "\n",
    "# Generate EDA report\n",
    "eda_report = eda.generate_eda_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b6fedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Geolocation Integration\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 3: GEOLOCATION INTEGRATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Initialize geolocation mapper\n",
    "geo_mapper = IPGeolocationMapper()\n",
    "\n",
    "# Load IP-to-country mapping\n",
    "ip_mapping_path = '../data/raw/IpAddress_to_Country.csv'\n",
    "geo_mapper.load_ip_country_mapping(ip_mapping_path)\n",
    "\n",
    "# Map IP addresses to countries\n",
    "df_fraud_geo = df_fraud_clean.copy()\n",
    "df_fraud_geo['country'] = geo_mapper.map_ips_to_countries(df_fraud_geo['ip_address'])\n",
    "\n",
    "# Analyze fraud by country\n",
    "fraud_by_country = geo_mapper.analyze_fraud_by_country(df_fraud_geo, ip_col='ip_address')\n",
    "\n",
    "# Validate mapping\n",
    "geo_mapper.validate_mapping()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9be284d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Feature Engineering\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 4: FEATURE ENGINEERING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Initialize feature engineer\n",
    "feature_engineer = FraudFeatureEngineer(verbose=True)\n",
    "\n",
    "# Create all features\n",
    "df_fraud_features = feature_engineer.create_all_features(df_fraud_geo, target_col='class')\n",
    "\n",
    "# Display feature statistics\n",
    "print(f\"\\nðŸ“ˆ Feature Engineering Statistics:\")\n",
    "print(feature_engineer.feature_stats)\n",
    "\n",
    "# Analyze feature importance\n",
    "importance_report = feature_engineer.get_feature_importance_report(\n",
    "    df_fraud_features, target_col='class', top_n=20\n",
    ")\n",
    "\n",
    "# Save engineered data\n",
    "output_path = '../data/processed/fraud_data_engineered.csv'\n",
    "df_fraud_features.to_csv(output_path, index=False)\n",
    "print(f\"\\nðŸ’¾ Engineered data saved to: {output_path}\")\n",
    "print(f\"   Shape: {df_fraud_features.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2e222a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Data Transformation\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 5: DATA TRANSFORMATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Initialize transformer\n",
    "transformer = FraudDataTransformer(random_state=42, verbose=True)\n",
    "\n",
    "# Run complete transformation pipeline\n",
    "transformed_data = transformer.full_pipeline(\n",
    "    df=df_fraud_features,\n",
    "    target_col='class',\n",
    "    normalize_method='standard',\n",
    "    encode_method='label',\n",
    "    imbalance_method='smote',\n",
    "    test_size=0.2\n",
    ")\n",
    "\n",
    "# Display transformation results\n",
    "X_train = transformed_data['X_train']\n",
    "X_test = transformed_data['X_test']\n",
    "y_train = transformed_data['y_train']\n",
    "y_test = transformed_data['y_test']\n",
    "\n",
    "print(f\"\\nâœ… Transformation Complete:\")\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"y_train fraud rate: {y_train.mean():.4f}\")\n",
    "print(f\"y_test fraud rate: {y_test.mean():.4f}\")\n",
    "\n",
    "# Save transformed data\n",
    "train_output = '../data/processed/X_train.csv'\n",
    "test_output = '../data/processed/X_test.csv'\n",
    "pd.DataFrame(X_train).to_csv(train_output, index=False)\n",
    "pd.DataFrame(X_test).to_csv(test_output, index=False)\n",
    "y_train.to_csv('../data/processed/y_train.csv', index=False)\n",
    "y_test.to_csv('../data/processed/y_test.csv', index=False)\n",
    "\n",
    "print(f\"\\nðŸ’¾ Transformed data saved to data/processed/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
